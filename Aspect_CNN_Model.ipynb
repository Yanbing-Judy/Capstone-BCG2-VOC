{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4tdyjEqs3_E1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import jieba\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "jieba.setLogLevel('WARN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "con1E4Sf4v-6"
   },
   "outputs": [],
   "source": [
    "#数据预处理\n",
    "class DataPreprocess():\n",
    "    def __init__(self, tokenizer=None,\n",
    "                 label_set=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_words = None\n",
    "        self.label_set = label_set\n",
    "        self.sentence_len = None\n",
    "        self.word_len = None\n",
    "\n",
    "    def cut_texts(self, texts=None, word_len=1):\n",
    "        \"\"\"\n",
    "        对文本分词\n",
    "        :param texts: 文本列表\n",
    "        :param word_len: 保留最短长度的词语\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if word_len > 1:\n",
    "            texts_cut = [[word for word in jieba.lcut(text) if len(word) >= word_len] for text in texts]\n",
    "        else:\n",
    "            texts_cut = [jieba.lcut(one_text) for one_text in texts]\n",
    "\n",
    "        self.word_len = word_len\n",
    "\n",
    "        return texts_cut\n",
    "\n",
    "    def train_tokenizer(self,\n",
    "                        texts_cut=None,\n",
    "                        num_words=2000):\n",
    "        \"\"\"\n",
    "        生成编码字典\n",
    "        :param texts_cut: 分词的列表\n",
    "        :param num_words: 字典按词频从高到低保留数量\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tokenizer = Tokenizer(num_words=num_words)\n",
    "        tokenizer.fit_on_texts(texts=texts_cut)\n",
    "        num_words = min(num_words, len(tokenizer.word_index) + 1)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.num_words = num_words\n",
    "\n",
    "    def text2seq(self,\n",
    "                 texts_cut,\n",
    "                 sentence_len=30):\n",
    "        \"\"\"\n",
    "        文本转序列，用于神经网络的ebedding层输入。\n",
    "        :param texts_cut: 分词后的文本列表\n",
    "        :param sentence_len: 文本转序列保留长度\n",
    "        :return:sequence list\n",
    "        \"\"\"\n",
    "        tokenizer = self.tokenizer\n",
    "        texts_seq = tokenizer.texts_to_sequences(texts=texts_cut)\n",
    "        del texts_cut\n",
    "\n",
    "        texts_pad_seq = pad_sequences(texts_seq,\n",
    "                                      maxlen=sentence_len,\n",
    "                                      padding='post',\n",
    "                                      truncating='post')\n",
    "        self.sentence_len = sentence_len\n",
    "        return texts_pad_seq\n",
    "\n",
    "    def creat_label_set(self, labels):\n",
    "        '''\n",
    "        获取标签集合，用于one-hot\n",
    "        :param labels: 原始标签集\n",
    "        :return:\n",
    "        '''\n",
    "        label_set = set()\n",
    "        for i in labels:\n",
    "            label_set = label_set.union(set(i))\n",
    "\n",
    "        self.label_set = np.array(list(label_set))\n",
    "\n",
    "    def creat_label(self, label):\n",
    "        '''\n",
    "        构建标签one-hot\n",
    "        :param label: 原始标签\n",
    "        :return: 标签one-hot形式的array\n",
    "        eg. creat_label(label=data_valid_accusations[12], label_set=accusations_set)\n",
    "        '''\n",
    "        label_set = self.label_set\n",
    "        label_zero = np.zeros(len(label_set))\n",
    "        label_zero[np.in1d(label_set, label)] = 1\n",
    "        return label_zero\n",
    "\n",
    "    def creat_labels(self, labels=None):\n",
    "        '''\n",
    "        调用creat_label遍历标签列表生成one-hot二维数组\n",
    "        :param label: 原始标签集\n",
    "        :return:\n",
    "        '''\n",
    "        label_set = self.label_set\n",
    "        labels_one_hot = [self.creat_label(label) for label in labels]\n",
    "\n",
    "        return np.array(labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVCKic1f5q_N",
    "outputId": "47a1d747-b53b-4d97-fc0d-ac28bc6f7872"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 10)]              0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 10, 10)            110       \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 10, 128)           3968      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 128)              0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 500)               64500     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 500)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 73,588\n",
      "Trainable params: 73,588\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#CNN模型\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import Conv1D, GlobalMaxPool1D, Dropout\n",
    "\n",
    "\n",
    "def CNN(input_dim,\n",
    "        input_length,\n",
    "        vec_size,\n",
    "        output_shape,\n",
    "        output_type='multiple'):\n",
    "    '''\n",
    "    Creat CNN net,use Embedding+CNN1D+GlobalMaxPool1D+Dense.\n",
    "    You can change filters and dropout rate in code..\n",
    "    :param input_dim: Size of the vocabulary\n",
    "    :param input_length:Length of input sequences\n",
    "    :param vec_size:Dimension of the dense embedding\n",
    "    :param output_shape:Target shape,target should be one-hot term\n",
    "    :param output_type:last layer type,multiple(activation=\"sigmoid\") or single(activation=\"softmax\")\n",
    "    :return:keras model\n",
    "    '''\n",
    "    data_input = Input(shape=[input_length])\n",
    "    word_vec = Embedding(input_dim=input_dim + 1,\n",
    "                         input_length=input_length,\n",
    "                         output_dim=vec_size)(data_input)\n",
    "    x = Conv1D(filters=128,\n",
    "               kernel_size=[3],\n",
    "               strides=1,\n",
    "               padding='same',\n",
    "               activation='relu')(word_vec)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dense(500, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    if output_type == 'multiple':\n",
    "        x = Dense(output_shape, activation='sigmoid')(x)\n",
    "        model = Model(inputs=data_input, outputs=x)\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])\n",
    "    elif output_type == 'single':\n",
    "        x = Dense(output_shape, activation='softmax')(x)\n",
    "        model = Model(inputs=data_input, outputs=x)\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics=['acc'])\n",
    "    else:\n",
    "        raise ValueError('output_type should be multiple or single')\n",
    "    return model\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model = CNN(input_dim=10, input_length=10, vec_size=10, output_shape=10, output_type='multiple')\n",
    "    model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iFYHpa745Oj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class TextClassification():\n",
    "    def __init__(self):\n",
    "        self.preprocess = None\n",
    "        self.model = None\n",
    "\n",
    "    def get_preprocess(self, texts, labels, word_len=1, num_words=2000, sentence_len=30):\n",
    "        # 数据预处理\n",
    "        preprocess = DataPreprocess()\n",
    "\n",
    "        # 处理文本\n",
    "        texts_cut = preprocess.cut_texts(texts, word_len)\n",
    "        preprocess.train_tokenizer(texts_cut, num_words)\n",
    "        texts_seq = preprocess.text2seq(texts_cut, sentence_len)\n",
    "\n",
    "        # 得到标签\n",
    "        preprocess.creat_label_set(labels)\n",
    "        labels = preprocess.creat_labels(labels)\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "        return texts_seq, labels\n",
    "\n",
    "    def fit(self, texts_seq, texts_labels, output_type, epochs, batch_size, model=None):\n",
    "        if model is None:\n",
    "            preprocess = self.preprocess\n",
    "            model = CNN(preprocess.num_words,\n",
    "                        preprocess.sentence_len,\n",
    "                        128,\n",
    "                        len(preprocess.label_set),\n",
    "                        output_type)\n",
    "        # 训练神经网络\n",
    "        model.fit(texts_seq,\n",
    "                  texts_labels,\n",
    "                  epochs=epochs,\n",
    "                  batch_size=batch_size)\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, texts):\n",
    "        preprocess = self.preprocess\n",
    "        word_len = preprocess.word_len\n",
    "        sentence_len = preprocess.sentence_len\n",
    "\n",
    "        # 处理文本\n",
    "        texts_cut = preprocess.cut_texts(texts, word_len)\n",
    "        texts_seq = preprocess.text2seq(texts_cut, sentence_len)\n",
    "\n",
    "        return self.model.predict(texts_seq)\n",
    "\n",
    "    def label2toptag(self, predictions, labelset):\n",
    "        labels = []\n",
    "        for prediction in predictions:\n",
    "            label = labelset[prediction == prediction.max()]\n",
    "            labels.append(label.tolist())\n",
    "        return labels\n",
    "\n",
    "    def label2half(self, predictions, labelset):\n",
    "        labels = []\n",
    "        for prediction in predictions:\n",
    "            label = labelset[prediction > 0.5]\n",
    "            labels.append(label.tolist())\n",
    "        return labels\n",
    "\n",
    "    def label2tag(self, predictions, labelset):\n",
    "        labels1 = self.label2toptag(predictions, labelset)\n",
    "        labels2 = self.label2half(predictions, labelset)\n",
    "        labels = []\n",
    "        for i in range(len(predictions)):\n",
    "            if len(labels2[i]) == 0:\n",
    "                labels.append(labels1[i])\n",
    "            else:\n",
    "                labels.append(labels2[i])\n",
    "        return labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mz21BBPch47X"
   },
   "source": [
    "## 属性分析\n",
    "### 以下使用的是1w条汽车VOC数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YHwxoATCFk5P"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df3 = pd.read_csv(\"train_data.csv\")\n",
    "df3.drop(['Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6','Unnamed: 7'],inplace=True,axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ElULXbsUh4jD",
    "outputId": "6241cc57-0648-4871-c86b-00dfeed7a7e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "134/134 [==============================] - 5s 31ms/step - loss: 1.5637 - acc: 0.4881\n",
      "Epoch 2/10\n",
      "134/134 [==============================] - 4s 31ms/step - loss: 0.7796 - acc: 0.7770\n",
      "Epoch 3/10\n",
      "134/134 [==============================] - 4s 31ms/step - loss: 0.5504 - acc: 0.8364\n",
      "Epoch 4/10\n",
      "134/134 [==============================] - 4s 31ms/step - loss: 0.4425 - acc: 0.8648\n",
      "Epoch 5/10\n",
      "134/134 [==============================] - 4s 31ms/step - loss: 0.3432 - acc: 0.8953\n",
      "Epoch 6/10\n",
      "134/134 [==============================] - 4s 31ms/step - loss: 0.2460 - acc: 0.9253\n",
      "Epoch 7/10\n",
      "134/134 [==============================] - 4s 31ms/step - loss: 0.1536 - acc: 0.9547\n",
      "Epoch 8/10\n",
      "134/134 [==============================] - 5s 37ms/step - loss: 0.0882 - acc: 0.9777\n",
      "Epoch 9/10\n",
      "134/134 [==============================] - 7s 56ms/step - loss: 0.0510 - acc: 0.9898\n",
      "Epoch 10/10\n",
      "134/134 [==============================] - 4s 31ms/step - loss: 0.0314 - acc: 0.9945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67/67 [==============================] - 0s 6ms/step\n",
      "[0.76396058]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_info = df3['content']\n",
    "y_info = [[i] for i in df3['label']]\n",
    "data_type = 'single'\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_info, y_info, test_size=0.2, random_state=1)\n",
    "\n",
    "##### 以下是训练过程 #####\n",
    "\n",
    "clf = TextClassification()\n",
    "texts_seq, texts_labels = clf.get_preprocess(x_train, y_train,\n",
    "                                             word_len=1,\n",
    "                                             num_words=2000,\n",
    "                                             sentence_len=50)\n",
    "clf.fit(texts_seq=texts_seq,\n",
    "        texts_labels=texts_labels,\n",
    "        output_type=data_type,\n",
    "        epochs=10,\n",
    "        batch_size=64,\n",
    "        model=None)\n",
    "\n",
    "# 保存整个模块,包括预处理和神经网络\n",
    "with open('./%s.pkl' % data_type, 'wb') as f:\n",
    "    pickle.dump(clf, f)\n",
    "\n",
    "##### 以下是预测过程 #####\n",
    "\n",
    "# 导入刚才保存的模型\n",
    "with open('./%s.pkl' % data_type, 'rb') as f:\n",
    "    clf = pickle.load(f)\n",
    "y_predict = clf.predict(x_test)\n",
    "y_predict = [[clf.preprocess.label_set[i.argmax()]] for i in y_predict]\n",
    "score = sum(y_predict == np.array(y_test)) / len(y_test)\n",
    "print(score) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8xmz7BQJF4KY",
    "outputId": "caeb06bf-5748-4448-d776-0451baef5479"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          价格       0.86      0.80      0.83       302\n",
      "          内饰       0.78      0.69      0.73        89\n",
      "          动力       0.83      0.79      0.81       596\n",
      "          外观       0.71      0.75      0.73       102\n",
      "         安全性       0.65      0.69      0.67       105\n",
      "          操控       0.72      0.62      0.67       246\n",
      "          油耗       0.80      0.85      0.82       254\n",
      "          空间       0.67      0.65      0.66        91\n",
      "         舒适性       0.63      0.76      0.69       168\n",
      "          配置       0.71      0.83      0.77       178\n",
      "\n",
      "    accuracy                           0.76      2131\n",
      "   macro avg       0.74      0.74      0.74      2131\n",
      "weighted avg       0.77      0.76      0.76      2131\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "print(classification_report(y_predict,y_test))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
